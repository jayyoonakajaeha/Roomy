import os
import json
import base64
import asyncio
from typing import List, Dict, Optional
from PIL import Image
from io import BytesIO
from fastapi import UploadFile
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, util
from .models import RepairAnalysisResult, DuplicateReportInfo, RepairResponse

# ==========================================
# ðŸ”§ Configuration & Mock DB
# ==========================================

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") # Must be set in .env
genai.configure(api_key=GOOGLE_API_KEY)

# Mock Database for Reports
# Structure: { id, building, floor, description, embedding (tensor or list), image_path }
REPAIR_REPORTS = [] 
NEXT_REPORT_ID = 1

# Lazy Load Models
_clip_model = None

def get_clip_model():
    global _clip_model
    if _clip_model is None:
        print("Loading CLIP Model...")
        # using a small model for demo speed: clip-ViT-B-32
        _clip_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32')
        print("CLIP Model Loaded.")
    return _clip_model

# ==========================================
# ðŸ§  AI Analysis (Gemini)
# ==========================================

async def analyze_image_with_gemini(image_bytes: bytes) -> RepairAnalysisResult:
    """
    Gemini 3 Flash to analyze image.
    Enforces Korean output and strict JSON structure.
    """
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash", # Using 1.5 Flash as stable alias or 'gemini-1.5-pro' if needed. 3-flash-preview might need specific name check.
        # User requested 3-flash, usually accessed via preview names or 1.5 updates. 
        # I will use 'gemini-1.5-flash' as it's the current "Flash" standard avail in most keys, 
        # or 'models/gemini-1.5-flash-latest'. If fails, will fallback.
        generation_config={
            "response_mime_type": "application/json",
            "response_schema": RepairAnalysisResult
        }
    )
    
    # Image Prep
    # Gemini API supports bytes via Part if utilizing proper client.
    # Simpler via PIL -> Blob mapping in SDK? SDK supports PIL Image directly.
    pil_img = Image.open(BytesIO(image_bytes))
    
    prompt = """
    ë‹¹ì‹ ì€ ì‹œì„¤ ê´€ë¦¬ ë° ì•ˆì „ ì ê²€ ì „ë¬¸ê°€ AIìž…ë‹ˆë‹¤. 
    ì œê³µëœ ì‚¬ì§„ì„ ë¶„ì„í•˜ì—¬ ì‹œì„¤ë¬¼ì˜ ê³ ìž¥ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    
    **ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.**

    **ìš°ì„ ìˆœìœ„ íŒë‹¨ ê¸°ì¤€ (Priority Logic)**:
    1. **CRITICAL (ê¸´ê¸‰, ì ìˆ˜ 9-10)**: 
       - ê±°ì£¼ìžì˜ ê±´ê°•/ì•ˆì „ ìœ„í˜‘ (ì˜ˆ: í•˜ìˆ˜/ë³€ê¸° ì—­ë¥˜, ê°€ìŠ¤ ëˆ„ì¶œ, ì „ì„  ë…¸ì¶œ, í˜„ê´€ë¬¸/ë³´ì•ˆìž¥ì¹˜ íŒŒì†).
       - ì£¼ê±° ë¶ˆê°€ëŠ¥ ìƒíƒœ (ë‹¨ì „, ë‹¨ìˆ˜).
    2. **HIGH (ë†’ìŒ, ì ìˆ˜ 7-8)**: 
       - í•„ìˆ˜ ìƒí™œ ê¸°ëŠ¥ ë§ˆë¹„ (ì˜ˆ: ë‚œë°©/ì—ì–´ì»¨ ê³ ìž¥, ëƒ‰ìž¥ê³  ê³ ìž¥, ì‹±í¬ëŒ€ ëˆ„ìˆ˜).
    3. **MEDIUM (ì¤‘ê°„, ì ìˆ˜ 4-6)**: 
       - ê¸°ëŠ¥ìƒ ë¶ˆíŽ¸í•˜ë‚˜ ìƒí™œ ê°€ëŠ¥ (ì˜ˆ: ë°©ë¬¸ íŒŒì†, ì‹íƒ ì˜ìž íŒŒì†, ì „ë“± 1ê°œ ë‚˜ê°).
       - *ë¹„êµ*: í™”ìž¥ì‹¤ ë¬¸ì´ ìœ„ê¸‰í•˜ê²Œ ë¶€ì„œì¡Œë”ë¼ë„, ì˜¤ìˆ˜ê°€ ì—­ë¥˜í•˜ëŠ” ë³€ê¸°ë³´ë‹¤ëŠ” ìš°ì„ ìˆœìœ„ê°€ ë‚®ìŠµë‹ˆë‹¤.
    4. **LOW (ë‚®ìŒ, ì ìˆ˜ 1-3)**: 
       - ë¯¸ê´€ìƒ ë¬¸ì œ (ì˜ˆ: ë²½ì§€ ì°¢ì–´ì§, ìŠ¤í¬ëž˜ì¹˜).

    **ë¶„ì„ í•­ëª©**:
    - category: ë°°ê´€(plumbing), ì „ê¸°(electric), ê°€êµ¬(furniture), êµ¬ì¡°(structure), ê°€ì „(appliance) ë“±.
    - item: êµ¬ì²´ì ì¸ ë¬¼ê±´ ëª…ì¹­.
    - issue: ë¬¸ì œ í˜„ìƒ.
    - severity: CRITICAL, HIGH, MEDIUM, LOW ì¤‘ íƒ1.
    - priority_score: 1~10 ì‚¬ì´ ì •ìˆ˜.
    - reasoning: ì™œ ì´ ì‹¬ê°ë„ì¸ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª… (í•œêµ­ì–´).
    - repair_suggestion: ìˆ˜ë¦¬ ë°©ë²• ì œì•ˆ (í•œêµ­ì–´).
    - description: ìƒí™© ìš”ì•½ (í•œêµ­ì–´).
    """
    
    response = model.generate_content([prompt, pil_img])
    
    # Parse JSON
    try:
        data = json.loads(response.text)
        return RepairAnalysisResult(**data)
    except Exception as e:
        print(f"Gemini JSON Parse Error: {e}, Raw: {response.text}")
        # Fallback
        return RepairAnalysisResult(
            category="unknown", item="unknown", issue="unknown", 
            severity="MEDIUM", priority_score=5, reasoning="ë¶„ì„ ì‹¤íŒ¨", 
            repair_suggestion="", description="ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        )

# ==========================================
# ðŸ” Duplicate Detection (CLIP)
# ==========================================

async def check_duplicates(image_bytes: bytes, building: str, floor: str, room_number: Optional[str] = None) -> List[DuplicateReportInfo]:
    model = get_clip_model()
    
    # 1. Image Embedding
    pil_img = Image.open(BytesIO(image_bytes))
    query_emb = model.encode(pil_img, convert_to_tensor=True)
    
    duplicates = []
    
    # 2. Search in Mock DB
    for report in REPAIR_REPORTS:
        # Location Filter (Building & Floor match required)
        if report['building'] != building or report['floor'] != floor:
            continue
        
        # Room Filter
        # If input has room (Private), match exact room.
        # If input no room (Public), match reports with no room.
        report_room = report.get('room_number')
        if room_number != report_room:
            continue
            
        # Similarity
        if report.get('embedding') is not None:
            sim = util.pytorch_cos_sim(query_emb, report['embedding'])[0][0].item()
            
            # Threshold: 0.85 (High visual similarity)
            if sim >= 0.85:
                loc_str = f"{report['building']} {report['floor']}F"
                if report_room:
                    loc_str += f" {report_room}í˜¸"
                    
                duplicates.append(DuplicateReportInfo(
                    reportId=report['id'],
                    similarity=round(sim, 2),
                    description=report['description'],
                    location=loc_str
                ))
    
    # Sort by sim desc
    duplicates.sort(key=lambda x: x.similarity, reverse=True)
    return duplicates

async def save_report(analysis: RepairAnalysisResult, image_bytes: bytes, building: str, floor: str, room_number: Optional[str] = None):
    """
    In-memory save for future duplicate checks. 
    In real app, save image to S3/Disk and Embedding to VectorDB.
    """
    global NEXT_REPORT_ID
    
    model = get_clip_model()
    pil_img = Image.open(BytesIO(image_bytes))
    emb = model.encode(pil_img, convert_to_tensor=True)
    
    REPAIR_REPORTS.append({
        "id": NEXT_REPORT_ID,
        "building": building,
        "floor": floor,
        "room_number": room_number,
        "description": analysis.description,
        "embedding": emb
    })
    NEXT_REPORT_ID += 1

# ==========================================
# ðŸš€ Main Logic
# ==========================================

async def process_repair_request(file: UploadFile, building: str, floor: str, room_number: Optional[str] = None) -> RepairResponse:
    content = await file.read()
    
    # Parallelize? Gemini & CLIP
    # For now, sequential
    
    # 1. Analyze
    analysis_task = analyze_image_with_gemini(content)
    
    # 2. Check Duplicates (Only checks against *previously* saved reports)
    duplicates_task = check_duplicates(content, building, floor, room_number)
    
    analysis, duplicates = await asyncio.gather(analysis_task, duplicates_task)
    
    # 3. Save current report
    await save_report(analysis, content, building, floor, room_number)
    
    return RepairResponse(
        analysis=analysis,
        duplicates=duplicates,
        is_new=(len(duplicates) == 0)
    )
